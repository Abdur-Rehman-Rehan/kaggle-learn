{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up feedback system\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.ethics.ex4 import *\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data, separate features from target\n",
    "data = pd.read_csv(\"../input/synthetic-credit-card-approval/synthetic_credit_card_approval.csv\")\n",
    "X = data.drop([\"Target\"], axis=1)\n",
    "y = data[\"Target\"]\n",
    "\n",
    "# Break into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "# Preview the data\n",
    "print(\"Data successfully loaded!\\n\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train a model and make predictions\n",
    "model_baseline = tree.DecisionTreeClassifier(random_state=0, max_depth=3)\n",
    "model_baseline.fit(X_train, y_train)\n",
    "preds_baseline = model_baseline.predict(X_test)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(estimator, X, y_true, y_pred, display_labels=[\"Deny\", \"Approve\"],\n",
    "                          include_values=True, xticks_rotation='horizontal', values_format='',\n",
    "                          normalize=None, cmap=plt.cm.Blues):\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize=normalize)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "    return cm, disp.plot(include_values=include_values, cmap=cmap, xticks_rotation=xticks_rotation,\n",
    "                     values_format=values_format)\n",
    "\n",
    "# Function to evaluate the fairness of the model\n",
    "def get_stats(X, y, model, group_one, preds):\n",
    "        \n",
    "    y_zero, preds_zero, X_zero = y[group_one==False], preds[group_one==False], X[group_one==False]\n",
    "    y_one, preds_one, X_one = y[group_one], preds[group_one], X[group_one]\n",
    "    \n",
    "    print(\"Total approvals:\", preds.sum())\n",
    "    print(\"Group A:\", preds_zero.sum(), \"({}% of approvals)\".format(round(preds_zero.sum()/sum(preds)*100, 2)))\n",
    "    print(\"Group B:\", preds_one.sum(), \"({}% of approvals)\".format(round(preds_one.sum()/sum(preds)*100, 2)))\n",
    "    \n",
    "    print(\"\\nOverall accuracy: {}%\".format(round((preds==y).sum()/len(y)*100, 2)))\n",
    "    print(\"Group A: {}%\".format(round((preds_zero==y_zero).sum()/len(y_zero)*100, 2)))\n",
    "    print(\"Group B: {}%\".format(round((preds_one==y_one).sum()/len(y_one)*100, 2)))\n",
    "    \n",
    "    cm_zero, disp_zero = plot_confusion_matrix(model, X_zero, y_zero, preds_zero)\n",
    "    disp_zero.ax_.set_title(\"Group A\")\n",
    "    cm_one, disp_one = plot_confusion_matrix(model, X_one, y_one, preds_one)\n",
    "    disp_one.ax_.set_title(\"Group B\")\n",
    "    \n",
    "    print(\"\\nSensitivity / True positive rate:\")\n",
    "    print(\"Group A: {}%\".format(round(cm_zero[1,1] / cm_zero[1].sum()*100, 2)))\n",
    "    print(\"Group B: {}%\".format(round(cm_one[1,1] / cm_one[1].sum()*100, 2)))\n",
    "    \n",
    "# Evaluate the model    \n",
    "get_stats(X_test, y_test, model_baseline, X_test[\"Group\"]==1, preds_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your answer (Run this code cell to get credit!)\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, feature_names, class_names=[\"Deny\", \"Approve\"], impurity=False):\n",
    "    plot_list = tree.plot_tree(model, feature_names=feature_names, class_names=class_names, impurity=impurity)\n",
    "    [process_plot_item(item) for item in plot_list]\n",
    "\n",
    "def process_plot_item(item):\n",
    "    split_string = item.get_text().split(\"\\n\")\n",
    "    if split_string[0].startswith(\"samples\"):\n",
    "        item.set_text(split_string[-1])\n",
    "    else:\n",
    "        item.set_text(split_string[0])\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plot_list = visualize_model(model_baseline, feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your answer (Run this code cell to get credit!)\n",
    "q_2.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset with gender removed\n",
    "X_train_unaware = X_train.drop([\"Group\"],axis=1)\n",
    "X_test_unaware = X_test.drop([\"Group\"],axis=1)\n",
    "\n",
    "# Train new model on new dataset\n",
    "model_unaware = tree.DecisionTreeClassifier(random_state=0, max_depth=3)\n",
    "model_unaware.fit(X_train_unaware, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "preds_unaware = model_unaware.predict(X_test_unaware)\n",
    "get_stats(X_test_unaware, y_test, model_unaware, X_test[\"Group\"]==1, preds_unaware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your answer (Run this code cell to get credit!)\n",
    "q_3.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the value of zero_threshold to hit the objective\n",
    "zero_threshold = 0.11\n",
    "one_threshold = 0.99\n",
    "\n",
    "# Evaluate the model\n",
    "test_probs = model_unaware.predict_proba(X_test_unaware)[:,1]\n",
    "preds_approval = (((test_probs>zero_threshold)*1)*[X_test[\"Group\"]==0] + ((test_probs>one_threshold)*1)*[X_test[\"Group\"]==1])[0]\n",
    "get_stats(X_test, y_test, model_unaware, X_test[\"Group\"]==1, preds_approval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your answer (Run this code cell to get credit!)\n",
    "q_4.check()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
