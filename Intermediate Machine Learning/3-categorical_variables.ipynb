{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up code checking\n",
    "import os\n",
    "if not os.path.exists(\"../input/train.csv\"):\n",
    "    os.symlink(\"../input/home-data-for-ml-course/train.csv\", \"../input/train.csv\")  \n",
    "    os.symlink(\"../input/home-data-for-ml-course/test.csv\", \"../input/test.csv\") \n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.ml_intermediate.ex3 import *\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "X = pd.read_csv('../input/train.csv', index_col='Id') \n",
    "X_test = pd.read_csv('../input/test.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "X.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X.SalePrice\n",
    "X.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# To keep things simple, we'll drop columns with missing values\n",
    "cols_with_missing = [col for col in X.columns if X[col].isnull().any()] \n",
    "X.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_test.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n",
    "                                                      train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the lines below: drop columns in training and validation data\n",
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Check your answers\n",
    "step_1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in 'Condition2' column in training data:\", X_train['Condition2'].unique())\n",
    "print(\"\\nUnique values in 'Condition2' column in validation data:\", X_valid['Condition2'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your answer (Run this code cell to receive credit!)\n",
    "step_2.a.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns in the training data\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# Columns that can be safely ordinal encoded\n",
    "good_label_cols = [col for col in object_cols if \n",
    "                   set(X_valid[col]).issubset(set(X_train[col]))]\n",
    "        \n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "        \n",
    "print('Categorical columns that will be ordinal encoded:', good_label_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Drop categorical columns that will not be encoded\n",
    "label_X_train = X_train.drop(bad_label_cols, axis=1)\n",
    "label_X_valid = X_valid.drop(bad_label_cols, axis=1)\n",
    "\n",
    "# Apply ordinal encoder \n",
    "# Your code here\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[good_label_cols] = ordinal_encoder.fit_transform(X_train[good_label_cols])\n",
    "label_X_valid[good_label_cols] = ordinal_encoder.transform(X_valid[good_label_cols])\n",
    "\n",
    "\n",
    "# Check your answer\n",
    "step_2.b.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE from Approach 2 (Ordinal Encoding):\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of unique entries in each column with categorical data\n",
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the line below: How many categorical variables in the training data\n",
    "# have cardinality greater than 10?\n",
    "high_cardinality_numcols = 3\n",
    "\n",
    "# Fill in the line below: How many columns are needed to one-hot encode the \n",
    "# 'Neighborhood' variable in the training data?\n",
    "num_cols_neighborhood = 25\n",
    "\n",
    "# Check your answers\n",
    "step_3.a.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the line below: How many entries are added to the dataset by \n",
    "# replacing the column with a one-hot encoding?\n",
    "OH_entries_added = 1e4*100 - 1e4\n",
    "\n",
    "# Fill in the line below: How many entries are added to the dataset by\n",
    "# replacing the column with an ordinal encoding?\n",
    "label_entries_added = 0\n",
    "\n",
    "# Check your answers\n",
    "step_3.b.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that will be one-hot encoded\n",
    "low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "# Columns that will be dropped from the dataset\n",
    "high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "\n",
    "print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Use as many lines of code as you need!\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n",
    "\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "OH_X_valid.columns = OH_X_valid.columns.astype(str)\n",
    "\n",
    "# Check your answer\n",
    "step_4.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
